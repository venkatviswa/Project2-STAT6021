---
title: 'Stat 6021: Project 2'
author: 'Group 5: Etienne Jimenez, Bardia Nikpour, Christian Ollen,Venkat Viswanathan' 
output:
  pdf_document:
    extra_dependencies: ["amsmath", "amssymb", "amsthm", "mathtools"]
  html_document:
    df_print: paged
---


# Section 1

- Price Distribution: The broke in the price of housing is right skew, meaning that the many houses are distributed around lower price range, where the rest of expensive houses are located in the tail of the distribution. Premium homes, as shown within this data set, are outliers inferring.
- Bedroom Distribution: 3-bedroom houses are the biggest ones, while the number of houses is decreasing as the number of bedrooms is increasing in another one. Beyond enormous houses with more 8 bedrooms are not in usual cases and this is considered as an anomaly.
- Living Space and Price Correlation: A good correlation can be drawn between the living area size and the house price, however, such a relationship doesn't perfectly fit any specific pattern.
- Waterfront Rarity: Water frontage properties are indicating shortage; only 0.1% of the dataset being such, which might also justify higher price due to their rarity as a special commodity.
- Floor Preferences: Of the two predominant styles of homes, either single-story or two-story are typically the most prevalent. The fewer-floor homes of 3.5 storeys are almost as rare as hen's teeth, and this proves that many citizens like living in skyscrapers.
- View Quality: Its observed that although the number of home without any valuable view approximately equals the number of average(rated 0) views, the outcome of homes possessing higher-leveled (rated 1-2) view are far less.
- Condition and Price: Houses in better conditions are prone to satisfy the median price, however this rule is not always the case throughout the condition hierarchy.
- Year Built and Price: No very a strong correlation is present between property age and price which indicates that the more dominant factors might are some different other ones.
- Model Performance: We began our analysis with a comprehensive set of predictors in our linear regression model and gradually reduced the number of predictors by removing variables with insignificant effects on our model — sqft_basement, condition, and homes with 8 bedrooms and 1.5 floors. Data refinement resulted in the reduction of RMSE, MAE, and MAPE variables which demonstrated the improved model's predictive accuracy.
- Model Diagnostics: Diagnostic plots support overall a good fit but reveal some features which do not meet the linearity requirements, do not have identical variance and contain some outliers. These deserve further investigation.

Concisely, the analysis has shown that characteristic factors such as space and presence on waterfront and condition of home appear to be the major contributors to the determination of prices, while others do not seem to have consistent significance as pertains to the dataset. Model diagnostics point to the fact that regression models require careful attention to the underlying assumptions; and interaction terms help in characterizing the observed relationships between explanatory and response variables, which are the elements of real-world data. The AUC value will let us know how good the decision-making of the logistic regression model would be for the data set with imbalanced data classes. Through a thorough analysis, we come up with useful information to retail clientele, sellers, and real estate agents from the area of King County, WA.



# Section 2
Dataset is obtained from Kaggle. It contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015. This dataset has the following variables.

1.	id: Unique identifier for each house sale.

2.	date: Date of the sale.

3.	price: Sale price of the house.

4.	bedrooms: Number of bedrooms in the house.

5.	bathrooms: Number of bathrooms in the house.

6.	sqft_living: Square footage of the interior living space of the house.

7.	sqft_lot: Square footage of the land lot.

8.	floors: Number of floors (levels) in the house.

9.	waterfront: Indicates if the house has a waterfront view (0 for no, 1 for yes).

10.	view: Index from 0 to 4 representing the quality of the view from the property.

11.	condition: Index from 1 to 5 representing the overall condition of the house.

12.	grade: Index from 1 to 13 representing the overall grade given to the housing unit, based on King County grading system.

13.	sqft_above: Square footage of the interior living space above ground level.

14.	sqft_basement: Square footage of the interior living space below ground level.

15.	yr_built: Year the house was built.

16.	yr_renovated: Year the house was last renovated.

17.	zip code: Zip code of the area where the house is located.

18.	lat: Latitude coordinate of the house.

19.	long: Longitude coordinate of the house.

20.	sqft_living15: The square footage of interior housing living space for the nearest 15 neighbours.

21.	sqft_lot15: The square footage of the land lots of the nearest 15 neighbours.
The above-mentioned variables are the original variables


# Section 3

## Question 1

**What internal housing factors influence the price of homes in King County from May 2014 to May 2015? **

•	Response Variable: Home price

•	Motivation: This question aims to understand the relationship between various internal features of a house (such as bedrooms, bathrooms, square footage, floors, condition, grade, and others) and its sale price. Investigating these factors can provide insights into the determinants of housing prices in the King County area during the specified time.

## Question 2

**Does the presence of one or more of waterfront,Condition, View, Grade, Yr Built, Yr Renovated cause the price of house being sold above the median price? **

•	Response Variable: above_median

•	Motivation: This question aims to 


# Section 4

```{r message=FALSE, echo=FALSE}
# importing libraries
library(tidyverse)
library(caret)
library(car)
library(corrplot)
library(ROCR)
```


```{r include=FALSE}
Data<-read.csv("kc_house_data.csv", sep=",", header=TRUE)

Data<-Data %>% select(-id, -date)
compare<-Data$price > 450000
Data.new<-data.frame(Data, compare)

#convert to factor
condition_med<-Data.new$condition > 3
view_med<-Data.new$view > 0
grade_med<-Data.new$grade > 10
Data.new$waterfront<-factor(Data.new$waterfront)

#merge into the original data frame
factored<-data.frame(Data.new, condition_med, view_med, grade_med)

set.seed(6021)
sample.data<-sample.int(nrow(factored), floor(.50*nrow(factored)), replace = F)
train<-factored[sample.data, ]
test<-factored[-sample.data, ]
```



## Distribution of Price

```{r, echo=FALSE}

# histogram of price
ggplot(train, aes(x = price)) + geom_histogram(bins = 50, fill = "blue") + 
  labs(title = "Distribution of Price", x = "Price", y = "Frequency")

```

The distribution is highly right-skewed, with a large frequency of lower-priced items and very few high-priced ones. The majority of prices of the houses fall close to the lower end of the price spectrum, which means that higher prices are outliers in this particular dataset.

## Distribution of Bedrooms
```{r echo=FALSE}
# checking distribution of bedrooms
train %>% count(bedrooms)

```

The distribution of bedrooms in the dataset shows that three-bedroom houses are the most common, followed by those with four bedrooms. Houses with 33 bedrooms is only one, along with those with 11 and 10 bedrooms, appear to be outliers. To maintain data integrity, we will exclude these outliers from further analysis.

```{r echo=FALSE}
# removing outliers from bedroom column
train <- train %>% filter(bedrooms != 33, bedrooms != 11, bedrooms != 10)
```


## Distribution of Bathrooms

```{r echo=FALSE}
# checking distribution of bathrooms
train %>% count(bathrooms)

```

The distribution of bathrooms in the dataset shows that three-bathroom houses are the most common, followed by those with 3.5 bathrooms Houses with 8 bathrooms house listing is 2 one

## price vs Square feet Living
```{r echo=FALSE}
# scatterplot of price vs sqft_living
ggplot(train, aes(x = sqft_living, y = price)) + geom_point() + 
  labs(title = "Price vs Square feet Living", x = "Square feet Living", y = "Price")

```
The scatter plot shows the correlation between square feet of living space and the cost of the apartments. The fact that we have an upward trend, revealing that the increase in price is proportional to the rise in the area, while it's a bit different from linear, is another proof of this cost-effectiveness. On the flip-side, there are some data points with big living area that is drastically more expensive than the rest of the listing thereby, showing these could be luxury or premium properties. These values are plotted far from the core of data and form a series of clusters that are anchored at specific points.

## Prices vs Bedrooms

```{r echo=FALSE}
# boxplots of prices vs bedrooms
train$bedrooms <- factor(train$bedrooms, levels = 1:9)
ggplot(train, aes(x = bedrooms, y = price)) +
  geom_boxplot() +
  labs(title = "Price vs Bedrooms", x = "Bedrooms", y = "Price")

```



The boxplot represent the property prices distribution in line with the quantity of the bedrooms. The mu price continuously rises with the quality up to a number of bedrooms, after which it exactly fluctuates. The scatter plot of average prices sorted by category of bedrooms is such that the spread of each category's prices increase as the number of bedrooms increases, as shown by the longer boxes and whiskers, pointing to a bigger spread of property values. Particularly there are a great big number of outliers, especially in the upper area of rooms per property, which perhaps means that there are some properties, if many bedrooms have them, that are crushed against the mean of category, if it’s in rooms.

## Price vs Floors

```{r echo=FALSE}
# boxplot of price vs floors
train$floors <- factor(train$floors, levels = c(1, 1.5, 2, 2.5, 3, 3.5))
ggplot(train, aes(x = floors, y = price)) +
  geom_boxplot() +
  labs(title = "Price vs Floors", x = "Floors", y = "Price")
```

```{r include=FALSE}
# return bedrooms to be a numeric variable
train$bedrooms<-as.numeric(train$bedrooms)
```

There are a significant number of community properties with 1 floor and the total price for them normally falls within a wide range. However, the median price for these types of properties is the cheapest among all the categories. Two-story boxes on the balancing beam point to the significantly larger median price, and a large part of the tale is the numerous outliers-obviously expensive homes. Residential units with one-and-half, two-and-half, and three floors have very small representation of the dataset and their price spectral is a bit uneven, but in particular, there are a few outliers among the 3-floor home residences. The scarce type of 3.5 floor homes have the highest median prices. The spread range here is narrow which implies that these house types have a pricing consistency. Outliers are present in all categories of floors, suggesting that rare or special geo-spatial attributes may be contributing to higher pricing.

Condition vs Price

```{r echo=FALSE}
# boxplot of condition vs price
train$condition <- as.factor(train$condition)
ggplot(train, aes(x = condition, y = price)) +
  geom_boxplot() +
  labs(title = "Price vs Condition", x = "Condition", y = "Price")


```

Properties in conditions rated as 1 and 2 have a narrower range of prices with lower medians, suggesting they are generally less expensive. The median prices seem to increase slightly for properties rated in condition 3, and there's a wider spread of prices, with many outliers indicating some high-priced properties. Condition 4 and 5 properties also show a higher median price compared to lower-rated conditions, with condition 5 showing the most significant spread in prices, though not necessarily the highest median price. This could imply that while good condition may contribute to a higher price, other factors like location or size might also play a significant role in determining a property's value.


```{r echo=FALSE, message=FALSE}
train$condition <- as.numeric(as.factor(train$condition))
train$grade <- as.numeric(as.factor(train$grade))
train$floors <- as.numeric(as.factor(train$floors))
correlation_matrix <- train %>% select(price, sqft_living, sqft_basement, condition, grade, floors, sqft_above) %>% cor()

corrplot(correlation_matrix, method = "color", 
         type = "upper",
         order = "hclust",
         tl.col = "black",
         tl.srt = 25,
         addCoef.col = "black",
         col = colorRampPalette(c("#6D9EC1", "white", "#E46726"))(200))

# Add a title
#title("Correlation Matrix Heatmap")
```

The Correlation Matrix, a data map created from your information, shows us the relationships between various objects. The strength of the relationship between two items is indicated by each number on this map. A value around one indicates that those variables typically move in together, i.e., as one increases, the other does too. It appears as though they move in different directions when it is close to -1. We begin to see patterns as we examine the map, such as the significant correlation between cost and size—larger houses typically cost more. However, there are also subtle connections that we can miss initially. This map allows us to identify instances where two items are overly similar, which aids in selecting the most relevant data for our investigation. There is a weak positive association between latitude and price, as seen by the correlation between latitude and price of 0.31 when we examine the relationship between latitude, price, and long price. Price and longitude have a weak link, as indicated by the 0.02 correlation between long and price. Comparable to a treasure map that leads us through our data, indicating which avenues to explore and which to keep clear of, enabling us to make informed decisions and reveal what's contained inside.


# Section 5

The question is to what extent do internal housing factor affect the price of houses in King County, Washington? We are going to use linear regression to answer this question.

Our target variable is price and we are going to use all the variables in used to find the correlation between the price and other variables.


## Data Transformation and Initial Model Fitting

Here, the house prices (`price`) are log-transformed to normalize the data, which is a common practice when the response variable is skewed. This helps improve the accuracy of linear regression models. An initial linear model is then fitted using a variety of predictors such as the number of bedrooms, bathrooms, square footage, and more. This step is aimed at understanding how these variables impact the log of house prices.

### Regression Equation

$$
price =\beta_0+ \beta_1x_1 +\beta_2x_2 + \beta_3x_3 + \beta_4x_4 + \beta_5x_5 + \beta_6x_6 + \beta_7x_7 + \beta_8x_8
$$




```{r echo=FALSE}
# Transformations and initial model fitting
train$log_price <- log(train$price)
initial_model <- lm(log_price ~ bedrooms + bathrooms + sqft_living + floors + condition + grade + sqft_above + sqft_basement, data = train)
summary(initial_model)
```

## Identifying and Removing Aliased Coefficients

Aliasing occurs when two or more variables in the model are linearly dependent or highly correlated, making it impossible to determine their individual effects. This code identifies any aliased coefficients in the initial model, helping to identify redundant predictors that should be removed to avoid multicollinearity issues.

```{r echo=FALSE}
# Identify and remove aliased coefficients
alias_info <- alias(initial_model)
aliased_vars <- rownames(alias_info$Complete)
cat("Aliased variables:", aliased_vars, "\n")
```

## Remove Aliased Variables and Fit a Refined Model

Using the information from the previous step, this code creates a formula excluding the aliased variables and fits a refined model. This refined model should be more robust and provide clearer insights, as any linear dependencies among predictors have been removed.

```{r echo=FALSE}
# Remove aliased variables
#refined_formula <- reformulate(setdiff(names(coef(initial_model))[-1], aliased_vars), response = "log_price")
refined_model <- lm(log_price ~ bedrooms + bathrooms + sqft_living + floors + condition + grade + sqft_above , data = train)
summary(initial_model)
```

## Check for Multicollinearity Using VIF

Variance Inflation Factor (VIF) scores are calculated to assess multicollinearity, which occurs when predictor variables are highly correlated. High VIF scores indicate potential issues which can negatively impact the model’s performance and interpretability. 

```{r echo=FALSE}
# Now, we can check for multicollinearity using VIF
vif_result <- vif(refined_model)
cat("VIF Scores after resolving aliasing:\n")
print(vif_result)
```

## Model Refinement - Interaction Terms and Removing Non-Significant Predictors

In this step, the model is updated to include an interaction term between `bedrooms` and `bathrooms`, while removing the individual `bedrooms` and `floors` predictors. Interaction terms allow the model to capture complex relationships between variables while removing non-significant predictors helps streamline the model.

### Regression Equation

$$
price =\beta_0+ \beta_1x_1 +\beta_2x_2 + \beta_3x_3 + \beta_4x_4 + \beta_5x_5 + \beta_6x_6 + \beta_7x_7
$$

```{r echo=FALSE}
# Model refinement - Interaction terms and removing non-significant predictors
refined_model <- update(refined_model, . ~ . + bedrooms:bathrooms - bedrooms - floors)
cat("Model diagnostics:\n")
print(summary(refined_model))
```

## Checking Model Diagnostics

This code generates diagnostic plots for the refined model to evaluate its performance. The plots typically include residuals vs. fitted values, normal Q-Q plots, scale-location plots, and residuals vs. leverage plots. These plots help in assessing the assumptions of linear regression and identifying potential issues like non-linearity, heteroscedasticity, and influential points.

Residuals vs Fitted: This plot shows the residuals on the y-axis and the fitted values on the x-axis. It helps to check for non-linearity, unequal error variances, and outliers. If the points form a horizontal band, it indicates that the linearity assumption holds, while patterns or trends suggest model issues.

Q-Q (Quantile-Quantile) Plot: This plot compares the standardized residuals against a theoretical normal distribution. If the residuals follow a straight line, this suggests that they are normally distributed. Deviations from the line indicate departures from normality.

Scale-Location (or Spread-Location): This plot displays the square root of the standardized residuals against the fitted values. It helps to check for homoscedasticity (constant variance). A horizontal line indicates equal spread, while a fan or funnel shape suggests changing variance.

Residuals vs Leverage: This plot highlights influential data points. It shows the standardized residuals against leverage, a measure of how far each data point is from the average predictor value. Points with high leverage and high residuals can overly influence the model, indicated by Cook's distance lines.

```{r echo=FALSE}
# Checking model diagnostics
par(mfrow = c(2, 2))
plot(refined_model)
```
## Checking for Influential Observations, High Leverage Observations, and Outliers

This code identifies and removes influential observations that can skew the model’s results. High leverage points and outliers are identified based on the "hat" values, which measure how far each observation’s predictor values are from the average predictor values. Removing such influential points helps in achieving a more reliable model.

```{r echo=FALSE}
# Checking for influential observations, high leverage observations, and outliers
influence_measures <- influence.measures(refined_model)
high_influence <- influence_measures$infmat[, "hat"] > (2 * mean(influence_measures$infmat[, "hat"]))
train <- train[!high_influence, ]
summary(train)
```
## Re-Fitting the Model Without Influential Observations

After removing the influential observations, the model is refitted to the refined dataset. This step ensures that the model is not biased by any problematic data points.

```{r echo=FALSE}
# Re-fit the model without influential observations
final_model <- update(refined_model, data = train)
summary(final_model)
```

## Fit the Final Model on Training Data

The final model is fitted using only the training data. This step ensures that the model learns the relationships between predictors and the response variable without being biased by the testing data.

### Regression Equation


$$
price =\beta_0+ \beta_1x_1 +\beta_2x_2 + \beta_3 x_3 
$$


```{r echo=FALSE}
# Fit the final model on training data
final_model_fit <- lm(log_price ~ sqft_living + grade + condition, data = train)

cat("Final model diagnostics:\n")
print(summary(final_model_fit))

```

## Model Diagnostics on Final Model

The final model's diagnostic plots and summary statistics are generated to evaluate its performance on the training data. These diagnostics help identify potential issues before moving to the testing phase.

Residuals vs Fitted: The plot looks acceptable, with no clear patterns or trends. The points are evenly distributed around zero, indicating that the model's assumptions are generally met. There's an outlier labeled "707306" that might be worth investigating.

Q-Q (Quantile-Quantile) Plot: The residuals align well with the diagonal, indicating that they are approximately normally distributed. This suggests that the normality assumption holds, although the point labeled "707306" again stands out slightly, hinting at a possible outlier.

Scale-Location (or Spread-Location): The plot is relatively horizontal, suggesting consistent variance across fitted values, although there is a slight trend visible. The same outlier is visible here as well.

Residuals vs Leverage: The plot shows that most points have low leverage. These might be influential points, given their positions near the Cook's distance threshold.

```{r echo=FALSE}
# Model diagnostics on final model
par(mfrow = c(2, 2))
plot(final_model_fit)
```


## Evaluate Model Performance on Test Data
The final step involves evaluating the model's performance on the testing data. The root mean squared error (RMSE) and R-squared values are calculated to gauge the model's predictive accuracy and explanatory power. These metrics provide a clear picture of how well the model generalizes to new data.

### RMSE on Test Data
The Root Mean Squared Error (RMSE) calculated on the test data is printed on this line. 
The average discrepancy between the expected and actual values is measured by RMSE.
A model with lower RMSE values performs better in terms of prediction.

###R-squared
The value of R-squared, calculated using the test data, is printed on this line. 
R-squared shows how much of the variance in the dependent variable 'log_price' can be accounted for by the model's independent variables (predictors).
Greater R-squared values signify an improved model-data fit.

```{r echo=FALSE}
# Evaluate model performance on test data
test_predictions <- predict(final_model_fit, newdata = test)
test_residuals <- test$log_price - test_predictions
RMSE <- sqrt(mean(test_residuals^2))
R_squared <- summary(final_model_fit)$r.squared

# Output model performance metrics
cat("RMSE on test data:", RMSE, "\n")
cat("R-squared:", R_squared, "\n")
```




